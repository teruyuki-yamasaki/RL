{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTDojaHNpamfQLHTpCYC3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/teruyuki-yamasaki/e2129a0efbd7d9ff2954f903072309b7/rl_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pDTFMv0k0Yf"
      },
      "source": [
        "**強化学習の基本的なモデル**\n",
        "\n",
        "強化学習は、所与の環境の下でエージェントが行動パターンを最適化するという問題を扱う.　エージェントは方策$\\pi$に基いて行動$a$を決定し環境に対して働きかける.　すると環境の状態$s$が変化しその変化に応じてエージェントは報酬$r$を取得する.　ここで報酬とは、エージェントが環境に及ぼした行動の結果の（エージェントにとっての）良し悪しについての情報を一元化した実数値である.　エージェントは得られる報酬の総額を最大化するような方策の探索を行う．これが強化学習の基本的なモデルになる.　なお、強化学習では、問題を扱いやすくするために、以下のようなことを前提とする：\n",
        "- 環境モデルのマルコフ性：時点$t+1$の状態$s'$は時点$t$の状態$s$と行動$a$にのみ依存し、それより過去の情報には依存しない性質を有する.\n",
        "$$Pr\\{S_{t+1} = s' | S_0, A_0, S_1, A_1, \\cdots, S_{t-1}, A_{t-1},S_t, A_t\\} \n",
        "= Pr\\{S_{t+1} = s' | S_t, A_t\\}$$\n",
        "※時点$t$より過去の情報は$S_t$に組み込まれていると捉えることができる.\n",
        "\n",
        "- 状態$s$の下で行動$a$がなされたときに環境が次状態$s'$に遷移する過程がマルコフ確率過程によって決まる：\n",
        "$$p(s' | s, a) = Pr\\{S_{t+1}=s' | S_t = s, A_t = a\\}$$\n",
        "\n",
        "- 状態$s$の下で行動$a$がなされ環境が次状態$s'$に遷移したときにエージェントが得る報酬$r$もマルコフ確率過程により与えられ、その期待値はステップ$(s,a,s')$毎に決められる：\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "p(r|s,a,s') &=& Pr\\{R_{t+1} = r | S_t=s, A_t=a, S_{t+1} = s'\\} \\\\\n",
        "r_{s s'}^{a} &=& \\sum_r r Pr\\{R_{t+1} = r | S_t = s, A_t = a, S_{t+1} = s'\\} \n",
        "\\\\ &=& E[R_{t+1} = r | S_t = s, A_t = a, S_{t+1} = s']\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "- 方策$\\pi$：エージェントが状態$s$の下で行動$a$を選択する過程もマルコフ確率過程：\n",
        "$$\\pi (a | s) = Pr\\{A_t = a | S_t = s\\}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NUq6bCT7HT0"
      },
      "source": [
        "**価値関数の定義**\n",
        "\n",
        "早速、強化学習における最適化の指標となる報酬の総額について定義していく.\n",
        "\n",
        "環境モデル$p(s'|s,a)$、$p(r|s,a,s')$、 方策$\\pi(a|s)$の下で、ある状態・行動・報酬系列(history)\n",
        "$$\n",
        "\\begin{align}\n",
        "H = \\{(S_t, A_t, R_{t+1})\\}_{t=0}^{\\infty}\n",
        "= \\{S_0, A_0, R_1, S_1, A_1, R_2, \\cdots, S_t, A_t, R_{t+1}, \\cdots\\}\n",
        "\\end{align}\n",
        "$$\n",
        "を取得したとする. \\\\\n",
        "このとき、状態$S_t=s$以降に得られた報酬の時点tにおける現在価値の総和は\n",
        "$$\n",
        "\\begin{align}\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=1}^\\infty \\gamma^{k-1}R_{t+k}\n",
        "\\end{align}\n",
        "$$\n",
        "のように、$\\gamma$を割引率とする割引報酬和として与えられる.これを時点$t$以降の収益という. \\\\\n",
        "\n",
        "(※終端状態T(Terminal)が存在する場合は、$R_{T+k} = 0$ $(k = 1,2,\\cdots)$として\n",
        "$$\n",
        "\\begin{align}\n",
        "G_t = \\sum_{k=1}^{T-t} \\gamma^{k-1}R_{t+k} + \\sum_{k=T-t+1}^\\infty \\gamma^{k-1}R_{t+k} = \\sum_{k=1}^{T-t} \\gamma^{k-1}R_{t+k}\n",
        "\\end{align}\n",
        "$$\n",
        "とでき、収益の元の定義に内包されていることがわかる.)\n",
        "\n",
        "この定義から、収益は、所与の環境において方策$\\pi$に従って行動していったときに取得される報酬のサンプル値から決まる値、すなわち$\\pi$に依存した値であるといえる.\n",
        "\n",
        "よって、方策$\\pi$の下での各状態$s$の価値は、所与の環境において方策$\\pi$に従っていったときに状態$S_t = s$以降に得られる収益$G_t$の期待値\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v_\\pi(s) = E_\\pi [G_t | S_t = s]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "として定義される.　\n",
        "\n",
        "以上より、強化学習における最適化問題は、「すべての状態$s \\in S$において価値関数$v_\\pi(s)$が最大となるような方策$\\pi(a|s)$を求める」という問題になる."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HCuaz2j6o4m"
      },
      "source": [
        "**ベルマン方程式の導出**\n",
        "\n",
        "いま定義した価値関数$v_\\pi(s)$を扱いやすくするために、ベルマン方程式を導出していく.\n",
        "\n",
        "再帰性の利用：\n",
        "\n",
        "まず、価値関数$v_\\pi(s)$を求めるために、\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "G_t \n",
        "&=& R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \n",
        "\\\\ &=& R_{t+1} + \\gamma \\underbrace{(R_{t+2} + \\gamma R_{t+3} + \\cdots)}_{G_{t+1}}\n",
        "\\\\ &=& R_{t+1} + \\gamma G_{t+1}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "という再帰性を利用して変形していく.　期待値の線形性から、価値関数は、即時報酬についての項と次時点以降の収益の期待値についての項の二項に分解できる：\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "E_\\pi [G_t | S_t = s] \n",
        "& = & E_\\pi [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots|S_t = s]\n",
        "\\\\ & = & E_\\pi [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots)|S_t = s]\n",
        "\\\\ & = & E_\\pi [R_{t+1} + \\gamma G_{t+1} | S_t = s]\n",
        "\\\\ & = & E_\\pi [R_{t+1}|S_t =s] + \\gamma E_\\pi [G_{t+1} | S_t = s]\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "以下で、最終行の第一項と第二項をそれぞれみていく."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTHyU7GMBhXl"
      },
      "source": [
        "第一項：\n",
        "\n",
        "まず、第一項について.\n",
        "$$E_\\pi [R_{t+1}|S_t =s] = \\sum_{a \\in A} \\underbrace{ Pr\\{A_t=a|S_t = s\\} }_{\\pi(a|s)} \\underbrace{E_\\pi[R_{t+1}|S_t=s, A_t=a]}_{(*)}$$\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "(*)& = & \\sum_{s'\\in S}Pr\\{S_{t+1}=s'|S_t=s,A_t=a\\}E_\\pi[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s']\n",
        "\\\\ & = & \\sum_{s'\\in S}p(s'|s,a)E[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s'] \n",
        "\\\\ & = & \\sum_{s'\\in S}p(s'|s,a) r_{s s'}^{a}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "$$\\therefore E_\\pi [R_{t+1}|S_t =s] = \\sum_{a \\in A} \\pi(a|s)\\sum_{s'\\in S}p(s'|s,a) r_{s s'}^{a} = r_s^\\pi$$\n",
        "\n",
        "意味としてはそのままで、方策$\\pi$の下で状態$s$から1ステップ経たときに得る報酬の期待値を表している."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTlUl2AhBkTz"
      },
      "source": [
        "第二項：\n",
        "\n",
        "次に、第二項について.\n",
        "$$\n",
        "E_\\pi [G_{t+1} | S_t = s] = \\sum_{a \\in A} \\underbrace{ Pr\\{A_t=a|S_t = s\\} }_{\\pi(a|s)} \\underbrace{E_\\pi[G_{t+1}|S_t=s, A_t=a]}_{(**)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "(**) =  \\sum_{s' \\in S} \\underbrace{Pr\\{S_{t+1}=s'|S_t=s,A_t=a\\}}_{p(s'|s,a)}\\underbrace{E_\\pi[G_{t+1}|S_t=s, A_t=a, S_{t+1}=s']}_{E_\\pi[G_{t+1}|S_{t+1}=s']= E_\\pi[G_t|S_t=s']=v_\\pi(s')}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\therefore E_\\pi [G_{t+1} | S_t = s]  \n",
        "&=& \\sum_{a \\in A} \\pi(a|s)\\sum_{s' \\in S} p(s'|s,a)v_\\pi(s')\n",
        "\\\\ &=& \\sum_{s' \\in S} \\underbrace{\\left(\\sum_{a \\in A}\\pi(s'|s,a)p(s'|s,a)\\right)}_{P_{ss'}^\\pi}v_\\pi(s') \n",
        "\\\\ &=& \\sum_{s' \\in S} P_{ss'}^\\pi v_\\pi(s')\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "$P_{ss'}^\\pi$は、方策$\\pi$に従ったときに状態$s$から$s'$に遷移する確率である.　この第二項が意味するのは、次状態の価値関数の期待値（次状態以降に得られる収益の期待値の期待値）である.\n",
        "\n",
        "追記：\n",
        "$$\n",
        "P_{ss'}^\\pi = P^\\pi(s'|s) \n",
        "$$\n",
        "と表記するのが良い."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oPM6Nw8G4vi"
      },
      "source": [
        "**価値関数が従うベルマン方程式：**\n",
        "\n",
        "以上をまとめると、価値関数$v_\\pi(s)$は、1ステップ刻んだときに得られる即時報酬の期待値$r_s^\\pi$と、次状態の価値関数の期待値$\\sum_{s'\\in S}P_{ss'}^\\pi v_\\pi(s')$の現在価値($\\gamma$倍)の和として展開でき、次のようにまとめられる：\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "v_\\pi(s)\n",
        "&=&  r_s^\\pi + \\gamma \\sum_{s' \\in S} P_{ss'}^\\pi v_\\pi(s')\n",
        "\\\\ &=& \\sum_{a \\in A} \\pi(a|s)\\sum_{s'\\in S}p(s'|s,a) r_{s s'}^{a} + \\gamma \\sum_{a \\in A} \\pi(a|s)\\sum_{s' \\in S} p(s'|s,a)v_\\pi(s') \n",
        "\\\\ &=& \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S}p(s'|s,a)(r_{ss'}^a + \\gamma v_\\pi(s'))\n",
        "\\\\ \n",
        "\\end{eqnarray}\n",
        "$$\n",
        "これがすべての$s \\in S$について成り立っている. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWkHT7T2MTWN"
      },
      "source": [
        "**価値関数のベルマン方程式のベクトル方程式**：\n",
        "\n",
        "以上により、\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "[v^\\pi]_s &=& v_\\pi(s) \n",
        "\\\\ [R^\\pi]_s &=& r_s^\\pi\n",
        "\\\\ [P^\\pi]_{ss'} &=& P_{ss'}^\\pi\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "となるような、方策$\\pi$のもとでの価値観数ベクトル$v^\\pi$、報酬ベクトル$R^\\pi$、状態遷移確率行列$P^\\pi$を用いて、すべての状態$s \\in S$のベルマン方程式をまとめて\n",
        "$$\n",
        "\\begin{align}\n",
        "v^\\pi = R^\\pi + \\gamma P^\\pi v^\\pi \n",
        "\\end{align}\n",
        "$$\n",
        "と表せる.　これは\n",
        "$$\n",
        "\\begin{align}\n",
        "v^\\pi = (I - \\gamma P^\\pi)^{-1} R^\\pi\n",
        "\\end{align}\n",
        "$$\n",
        "と解くことができる.　\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7tnPazPQ8Hi"
      },
      "source": [
        "**行動価値関数の導入**：\n",
        "\n",
        "方策を改善するためには、各状態における各行動の良し悪しを評価する指標を使う必要がある.\n",
        "そこで、方策$\\pi$の下で状態$s \\in S$のときに各行動$a \\in A$を取ったときの価値を表す行動価値関数を\n",
        "$$\n",
        "\\begin{align}\n",
        "q_\\pi(s,a) = E_\\pi[G_t|S_t=S,A_t=a]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "と定義する.　\n",
        "\n",
        "この定義より、価値関数$v_\\pi(s)$は、状態$s$において方策$\\pi(a|s)$に従って行動選択した際の行動価値関数$q_\\pi(s,a)$の期待値となっていることがわかる.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) q_\\pi(s,a)\n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGDvoVTmWBGJ"
      },
      "source": [
        "**行動価値関数が従うベルマン方程式の導出**：\n",
        "\n",
        "状態価値関数のときと同様の変形を行って\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "E_\\pi[G_t|S_t=s,A_t=a] \n",
        "&=& E_\\pi[R_{t+1}|S_t=S,A_t=a] + \\gamma E_\\pi[G_{t+1}|S_t=S,A_t=a]\n",
        "\\\\ &=& \\sum_{s' \\in S} p(s'|s,a) r_{ss'}^a + \\gamma \\sum_{s' \\in S} p(s'|s,a)v_\\pi(s')\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "とできる.\n",
        "\n",
        "すなわち、行動価値関数は状態価値関数を用いて\n",
        "$$\n",
        "\\begin{align}\n",
        "q_\\pi(s,a) = \\sum_{s' \\in S} p(s'|s,a) \\left(r_{ss'}^a + \\gamma v_\\pi(s')\\right)\n",
        "\\end{align}\n",
        "$$\n",
        "と表せる.　これが行動価値関数が従うベルマン方程式である."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFnAKAoQW8Ay"
      },
      "source": [
        "**方策反復法：greedyな方策改善**\n",
        "\n",
        "\n",
        "方策$\\pi$の下での各状態$s \\in S$における最適行動$a_{\\pi,s}^*$は、各状態行動対(s,a)の行動価値関数$q_\\pi(s,a)$を比較して\n",
        "$$\n",
        "\\begin{align}\n",
        "a_{\\pi,s}^* = \\mbox{argmax}_{a \\in A} q_\\pi(s,a)\n",
        "\\end{align}\n",
        "$$\n",
        "により決定される.\n",
        "これに基いて、greedyに方策を更新する：\n",
        "$$\n",
        "\\begin{align}\n",
        "\\pi_{old}(a|s) \\rightarrow \\pi_{new}(a|s) = \\left\\{ \n",
        "  \\begin{array}{cc}\n",
        "  1 / |A_{\\pi_{old}}^*(s)|, & a \\in A_{\\pi_{old}}^*(s) \\\\ \n",
        "  0, & \\mbox{otherwise}\n",
        "  \\end{array} \\right.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "ただし、$A_{\\pi_{old}}^*(s)$は更新前の方策$\\pi_{old}$の下での状態$s$における最適行動$a_{\\pi_{old},s}^*$の集合であり、$A(s)$の部分集合:\n",
        "$$A_{\\pi_{old}}^*(s) = \\left\\{ a^* \\in A(s)| a^* = \\mbox{argmax}_a q_{\\pi_{old}}(s,a)\\right\\}$$\n",
        "\n",
        "以上のように更新した新たな方策$\\pi_{new}$の下で、また新たに価値関数の導出、行動価値関数の導出、方策更新を行う.　\n",
        "この操作を繰り返すと方策は更新されなくなり、最適方策$\\pi_*$に収束する.\n",
        "※最適方策への収束の証明については別途参照."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlQs46Awcfm4"
      },
      "source": [
        "**最適ベルマン方程式の導出：**\n",
        "\n",
        "最適状態の定義：\n",
        "$$\n",
        "\\begin{align}\n",
        "\\pi_* = \\mbox{argmax}_\\pi v_\\pi(s),\\quad \\mbox{for all} \\ s \\in S\n",
        "\\end{align}\n",
        "$$\n",
        "のとき、方策$\\pi_*$は最適方策である.\n",
        "\n",
        "また、最適価値関数、最適行動価値関数はそれぞれ\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "v_*(s) &=& \\mbox{max}_\\pi v_\\pi(s) \\\\\n",
        "q_*(s,a) &=& \\mbox{max}_\\pi q_\\pi(s,a)\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "として定義される.\n",
        "\n",
        "最適ベルマン方程式の導出：\n",
        "\n",
        "価値関数と行動価値関数の関係式にこれらを代入すると\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "v_*(s) \n",
        "&=& \\sum_{a \\in A}\\pi_*(a|s) q_*(s,a)\n",
        "\\\\ &=& \\sum_{a_{\\pi_*, s}^* \\in A(s)}\\pi_*(a_{\\pi_*,s}^*|s) q_*(s,a_{\\pi_*,s}^*) \n",
        "\\\\ &=& |A^*(s)|\\frac{1}{|A^*(s)|}\\mbox{max}_{a}q_*(s,a) = \\mbox{max}_a q_*(s,a) \n",
        "\\end{eqnarray}\n",
        "$$\n",
        "すなわち、\n",
        "$$\n",
        "\\begin{align}\n",
        "v_*(s) = \\mbox{max}_a r_\\pi(s,a) \n",
        "\\end{align}\n",
        "$$\n",
        "となる.　ただし、\n",
        "$$\n",
        "\\begin{align}\n",
        "a_{\\pi_*,s}^*  =  \\mbox{argmax}_s q_*(s,a)\n",
        "\\end{align}\n",
        "$$\n",
        "また、行動価値関数のベルマン方程式より\n",
        "$$\n",
        "\\begin{align}\n",
        "q_*(s,a) = \\sum_{s' \\in S}p(s'|s,a)\\left(r_{ss'}^a + \\gamma v_*(s') \\right) \n",
        "\\end{align}\n",
        "$$\n",
        "が得られる.　\n",
        "\n",
        "以上をまとめると、最適状態において\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "q_*(s,a) & = & \\sum_{s' \\in S}p(s'|s,a)\\left(r_{ss'}^a + \\gamma v_*(s') \\right) \\\\\n",
        "v_*(s)  & = & \\mbox{max}_a q_*(s,a) \\\\\n",
        "a_{\\pi_*,s}^* & = & \\mbox{argmax}_a q_*(s,a)\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "が成り立っている．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl-IsJynnCvj"
      },
      "source": [
        "**価値反復法：**\n",
        "\n",
        "方策反復法においてベルマン方程式の解析解を求める計算コストが大きいときは、代わりに、最適ベルマン方程式の形のをした漸化式に適当な初期値を代入して逐次計算を行い極限を求めることによって最適状態を求める.\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "q_{k+1}(s,a) & = & \\sum_{s' \\in S}p(s'|s,a)\\left(r_{ss'}^a + \\gamma v_k(s') \\right) \\\\\n",
        "v_{k+1}(s)  & = & \\mbox{max}_a q_{k+1}(s,a) \\\\\n",
        "a_{\\pi_k,s}^* & = & \\mbox{argmax}_a q_{k+1}(s,a) \\\\\n",
        "\\end{eqnarray}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aHDghshcfhh"
      },
      "source": [
        "環境モデルが既知のときは、以上のように、状態・行動系列の全ての分岐について期待値計算を行うことができ、それに基づいて方策反復法あるいは価値反復法を用いて最適方策を決定することができることをみた（動的計画法： dynamic programming）.　次に、環境モデルが未知の場合に方策を最適化する方法について議論する.　"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-zH5ZRMdEMs"
      },
      "source": [
        "環境モデルが未知のときは、全条件分岐について厳密に期待値計算を行うことはできないため、動的計画法のような方法で最適方策を求めることはできない.　そのため、エージェントは、モデル未知の所与の環境において繰り返し行動選択と報酬取得を繰り返し、状態・行動・報酬系列をサンプリングしつつ探索的に最適方策を学習する必要がある.　\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KiJKyFWv0Ff"
      },
      "source": [
        "**モンテカルロ法(Monte Carlo, MC)**:\n",
        "\n",
        "エピソードを最初から最後まで走らせ、得られた状態・行動・報酬系列から、各状態・行動に対する収益$G_t$のサンプリングして価値関数または行動価値関数を推定する方法.　教科書では$V$を推定しているが、ここでは$Q$を推定する.\n",
        "アルゴリズムは以下の通り：\n",
        " - 各状態行動対の行動価値関数の推定値$Q(s,a)$を初期化する.\n",
        " - 未知の環境の下でエピソードをサンプリング: \n",
        " $$\\{S_0, A_0, R_1, S_1, A_1, R_2, \\cdots, S_{T-1}, A_{T-1}, R_{T},S_T\\}$$\n",
        " - 得られた系列から、各エピソードにおける各時点t以降に得られる収益$G_t$を求める.\n",
        " $$G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t-1} R_T$$\n",
        " - 各時点tでのステップごとに、全ての状態行動対$(s,a)$について\n",
        " $$\n",
        " \\begin{array}{ll}\n",
        " N_{t+1}(s,a) = N_t(s,a) + 1(S_t=s,A_t=a) \\\\\n",
        " Q_{t+1}(s,a) = \\frac{1}{N_{t+1}(s,a)}\\sum_{k=0}^t G_k 1(S_k=s, A_k=a)\n",
        " \\end{array}\n",
        " $$\n",
        " と更新していく.　\n",
        " - ひとつのエピソードについて更新が終わったら、また別のエピソードをサンプリングして同じことを繰り返す.\n",
        " - 各々の$G_t$は$q_\\pi(s,a)=E_\\pi[G_t|S_t=s, A_t = a]$を期待値とする確率変数だから、上記のような更新を繰り返して、$Q_\\infty(s,a) \\rightarrow q_\\pi(s,a)$により行動価値関数が求まることになる.\n",
        " \n",
        " ※every visitとevery first visitの区別があるようだが、ここでは扱わない.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-ukmaWRJEq1"
      },
      "source": [
        "- ここで、$Q$の更新式の右辺は\n",
        " $$\n",
        " \\begin{eqnarray}\n",
        " \\frac{1}{N_{t+1}(s,a)}\\sum_{k=0}^t G_k 1(S_k=s, A_k=a)\n",
        " &=&  \\frac{N_{t+1}(s,a) - 1(S_t=s,A_t=a)}{N_{t+1}(s,a)}\\frac{1}{N_{t}(s,a)}\\sum_{k=0}^{t-1} G_k 1(S_k=s, A_k=a) + \\frac{1}{N_{t+1}(s,a)}G_t 1(S_t=s,A_t=a)\n",
        " \\\\ &=& \\left( 1 - \\frac{1(S_t=s,A_t=a)}{N_{t+1}(s,a)}\\right)Q_t(s,a) +\\frac{1}{N_{t+1}(s,a)} G_t 1(S_t=s,A_t=a) \n",
        " \\\\ &=& Q_t(s,a) + \\frac{1}{N_{t+1}(s,a)}\\left(G_t - Q_t(s,a)\\right)1(S_t=s,A_t=a)\n",
        " \\end{eqnarray}\n",
        " $$\n",
        "と変形できるので、$Q$の更新式は\n",
        " $$\n",
        " Q_{t+1}(s,a) = Q_t(s,a) + \\frac{1}{N_{t+1}(s,a)}\\left(G_t - Q_t(s,a)\\right)1(S_t=s,A_t=a)\n",
        " $$\n",
        " と表すこともできる.\n",
        "- このことから、上述のように各ステップ毎に全ての状態行動対$(s,a)$について更新するのではなく、時点$t$で$(S_t,A_t)=(s,a)$となった状態行動対についてだけ\n",
        " $$\n",
        " \\begin{array}{ll}\n",
        " N(s,a) \\leftarrow N(s,a) + 1 & \\\\\n",
        " \\alpha_{s,a} \\leftarrow 1 / N(s,a) & \\\\\n",
        " Q(s,a) \\leftarrow Q(s,a) + \\alpha_{s,a} \\left(G_t - Q(s,a)\\right) & \n",
        " \\end{array}\n",
        " $$\n",
        " となるように情報を更新する方法に置き換えられる.\n",
        " - このアルゴリズムは、「真の値$q_\\pi(s,a)$のサンプル値$G_t$を得る度に、推定値$Q(s,a)$が目標値（のサンプル）$G_t$に近づくように重み$\\alpha_{s,a}$分だけ推定値$Q(s,a)$を更新する」ものと解釈でき、機械学習っぽい形式.\n",
        " - 各々の$G_t$は$q_\\pi(s,a)$を期待値として環境モデル(未知)と方策$\\pi$に従って分布する確率変数だから、そのときそのときの$G_t$に向かって重み$\\alpha$分だけ$Q(s,a)$を近づけるという操作を繰り返すことで、長期的視点に立てば、$Q(s,a)$は$G_t$の期待値$q_\\pi(s,a)$に近づいていくと考えられる.\n",
        " - この更新の仕方だと、ステップ$t$以前の情報は現在$t$の$N$、$Q$に上書きされていくので、最初の更新方法のように、過去の$S_t$や$A_t$の履歴を保持し更新のたびに毎回参照するような必要がない."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvJtZuyIWgbt"
      },
      "source": [
        "MCの問題点:\n",
        "- $G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t+1} R_T$を目標値として使うため、\n",
        " - 終端状態までの系列がサンプリングできるまで更新できない.\n",
        " - 必ず各エピソードが終端状態に達するモデルにしか適用できない.　\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsi2YqS9XixB"
      },
      "source": [
        "**TD学習**：\n",
        "\n",
        "- 1 step収益$G_t^{(1)}$を目標値とした学習手法：\n",
        "$$G_t^{(1)} = R_{t+1} + \\gamma Q(S_{t+1},A_{t+1})$$\n",
        "\n",
        "- この目標値は、本来の目標値（のサンプル値）\n",
        "$$\\begin{eqnarray}\n",
        "G_t &=& R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t+1} R_T\n",
        "\\\\ &=& R_{t+1} + \\gamma G_{t+1}\n",
        "\\end{eqnarray}$$\n",
        "のうち、\n",
        " - 状態行動対$(S_t,A_t)$からの1ステップ先の即時報酬$R_{t+1}$(と$S_{t+1}$)だけを実際にサンプリング\n",
        "  - 残りの$G_{t+1}$は、$Q_t(S_{t+1},A_{t+1})$(現状の推定値$Q_t$を使って推定した、現状の方策の下での状態$S_{t+1}$以降の収益の期待値)によって代替\n",
        "  - $A_{t+1}$は、方策$\\pi$に従ってサンプリング(SARSA)　または　$A_{t+1} = \\mbox{argmax}_{a'} Q(S_{t+1},a')$ により決定(Q学習) \\\\\n",
        "\n",
        "- 各ステップごとに、全ての状態行動対$(s,a)$について次のように$Q$関数を更新する：\n",
        "\n",
        "$$\n",
        "\\delta_{t+1} = R_{t+1} + \\gamma Q_t(S_{t+1},A_{t+1}) - Q_t(S_t,A_t) \\\\\n",
        "Q_{t+1}(s,a) = Q_t(s,a) +  \\alpha \\delta_{t+1}1(S_t=s, A_t=a)\n",
        "$$\n",
        "\n",
        "- このときの目標値と推定値の誤差$\\delta_{t+1}$を1-stepTD誤差(TD：temporal differnce)と呼ぶ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOqPxJRjG1h"
      },
      "source": [
        "- 更新のために必要なサンプリングは1ステップのみなのでオンライン学習が可能\n",
        "- 更新ごとに行うサンプリングは1ステップ分のみであるとはいえ、$R_{t+1}$、$S_{t+1}$は、未知の環境モデルの確率変数を与えてくれるサンプル値である.　よって、更新を繰り返せば、$Q(s,a)$には、環境と行動の確率的な相互作用についての情報が蓄積されていく.\n",
        "- しかし、やはり、サンプリングが1step分だけではbiasが大きくなることが問題である."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIsSCNpUmj7t"
      },
      "source": [
        "**n-step TD学習:**\n",
        "\n",
        "- MCと1stepTDの間を取り持つ手法\n",
        " - 1 step TDのサンプリング不足を補う\n",
        " - MCのエピソード全体のサンプリングができてはじめて更新ができるという制約を緩和\n",
        "- n-stepTD学習の目標値は、n-step収益：\n",
        "\n",
        "$$\\begin{eqnarray}\n",
        "G_t^{(n)} \n",
        "&=& R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma^{n-1}r_{t+n} + \\gamma^n Q_t(S_{t+n},A_{t+n})\n",
        "\\end{eqnarray}$$\n",
        "\n",
        "- すなわち、状態行動対$(S_t,A_t)$以降の収益の本来のサンプル値\n",
        "$$\\begin{eqnarray}\n",
        "G_t &=& R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t+1} R_T\n",
        "\\\\ &=& R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n G_{t+n}\n",
        "\\end{eqnarray}$$\n",
        "のうち、\n",
        " - 状態行動対$(S_{t+1},A_{t+1})から$n-step先までの報酬$R_{t+1}, \\cdots, R_{t+n}$を実際にサンプリング\n",
        " - それ以降の収益$G_{t+n}$は、時点$t+n$の状態$S_{t+n}$と現状の$Q$関数から推定した「$G_{t+n}$の期待値」$Q(S_{t+n},A_{t+n})$で代替\n",
        "\n",
        "- 各時点$t$において、状態行動対$(S_t,A_t)$から現状の方策と$Q$関数、未知の環境モデルの下で、$n$ステップ先までサンプリングして$G_t^{(n)}$を得たら、時点$t$に戻ってきて（?）全ての状態行動対$(s,a)$について\n",
        "$$\n",
        "Q_{t+1} (s,a) = Q_t(s,a) + \\alpha \\left( G_t^{(n)} - Q_t(S_t,A_t)\\right)1(S_t=s,A_t=a)\n",
        "$$\n",
        "により$Q$関数を更新する．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPG__A95s4by"
      },
      "source": [
        "- $n \\rightarrow \\infty$としたものがMCに対応\n",
        "- $n = 1$としたものが1stepTDに対応\n",
        "- 場合によって最適な$n$が異なる\n",
        "- MCほど待たなくて良いが、オンライン学習ができない$(n \\geq 2)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG8axecEu3IR"
      },
      "source": [
        "**TD($\\lambda$)法**:\n",
        "\n",
        "- 目標値を$\\lambda$-収益：\n",
        "$$\n",
        "G_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^{\\infty}\\lambda^{n-1}G_t^{(n)}\n",
        "$$\n",
        "とする.\n",
        " - 各$G_t^{(n)}$を$\\lambda^{n-1}$により重みづけして足し上げ($0 \\leq \\lambda \\leq 1$)\n",
        " - $1-\\lambda$は正規化因子\n",
        " $$\n",
        " (1 -\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G = (1-\\lambda) \\frac{1}{1 - \\lambda} G = G\n",
        " $$\n",
        "- 各時点$t$において、全ての状態行動対$(s,a)$について\n",
        "$$\n",
        "Q_{t+1} (s,a) = Q_t(s,a) + \\alpha \\left( G_t^\\lambda - Q_t(S_t,A_t)\\right)1(S_t=s,A_t=a)\n",
        "$$\n",
        "によって更新.\n",
        "\n",
        "- TD($\\lambda$)誤差：\n",
        "$$\n",
        "G_t^{\\lambda} - Q_t(S_t,A_t) = (1-\\lambda)\\sum_{n=1}^\\infty \\lambda^{n-1}\\left(G_t^{(n)} - Q_t(S_t,A_t)\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWAmIHHS7XU6"
      },
      "source": [
        "**n-step TD誤差の展開：**\n",
        "\n",
        "$$\\begin{eqnarray}\n",
        "G_t^{(n)} - Q_t(S_t,A_t) \n",
        "&=& \\sum_{k=1}^{n}\\gamma^{k-1}R_{t+k} + \\gamma^n Q_t(S_{t+n}, A_{t+n}) - Q_t(S_t,A_t)\n",
        "\\\\ &=& \\sum_{k=1}^{n}\\gamma^{k-1}\\left\\{\\delta_{t+k} - \\gamma Q_{t+k-1}(S_{t+k},A_{t+k}) + Q_{t+k-1}(S_{t+k-1},A_{t+k-1})\\right\\} + \\gamma^n Q_t(S_{t+n}, A_{t+n}) - Q_t(S_t,A_t)\n",
        "\\\\ &=& \\sum_{k=1}^{n}\\gamma^{k-1}\\delta_{t+k} - \\underbrace{\\sum_{k=1}^{n}\\gamma^k Q_{t+k-1}(S_{t+k},A_{t+k})}_{\\sum_{k=1}^{n-1}\\gamma^k Q_{t+k-1}(S_{t+k},A_{t+k}) + \\gamma^n Q_{t+n-1}(S_{t+n},A_{t+n})} + \\underbrace{\\sum_{k=1}^{n}\\gamma^{k-1}Q_{t+k-1}(S_{t+k-1},A_{t+k-1})}_{Q_t(S_t,A_t) + \\sum_{k=1}^{n-1}\\gamma^kQ_{t+k}(S_{t+k},A_{t+k})} + \\gamma^n Q_t(S_{t+n}, A_{t+n}) - Q_t(S_t,A_t)\n",
        "\\\\ &=& \\sum_{k=1}^{n}\\gamma^{k-1}\\delta_{t+k} +  \\sum_{k=1}^{n-1}\\gamma^k \\left\\{Q_{t+k}(S_{t+k},A_{t+k})-Q_{t+k-1}(S_{t+k},A_{t+k}) \\right\\} + \\gamma^n \\left\\{Q_t(S_{t+n},A_{t+n}) - Q_{t+n-1}(S_{t+n},A_{t+n}) \\right\\}\n",
        "\\end{eqnarray}$$\n",
        "- ただし、\n",
        "$$\n",
        "\\delta_{t+k} = R_{t+k} + \\gamma Q_{t+k-1}(S_{t+k},A_{t+k}) - Q_{t+k-1}(S_t,A_t)\n",
        "$$\n",
        "は時点$t+k-1$から$t+k$への1stepTD誤差である.\n",
        "- $Q$関数がステップごとに逐次更新されるときは、第二項、第三項における$Q$関数の差分は$0$にならず、学習率$\\alpha$に比例する. \n",
        "- $Q$関数をステップごとに更新せずにnステップ分のサンプルを取って計算するときは補正項$O(\\alpha)$は現れない.\n",
        "- よって、$O(\\alpha)$を無視する近似において、時点$t$におけるn-stepTD誤差は\n",
        "$$\n",
        "G_t^{(n)} - Q_t(S_t,A_t) = \\sum_{k=1}^{n}\\gamma^{k-1}\\delta_{t+k} + O(\\alpha)\n",
        "$$\n",
        "のように、時点$t$以降のn個の1stepTD誤差の割引級数和として展開できる."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqE_I2wermMc"
      },
      "source": [
        "$$\\begin{eqnarray}\n",
        "G_t^{\\lambda} - Q_t(S_t,A_t) \n",
        "&=& (1-\\lambda)\\sum_{n=1}^\\infty \\lambda^{n-1}\\left(G_t^{(n)} - Q_t(S_t,A_t)\\right) \n",
        "\\\\ &=& (1-\\lambda)\\sum_{n=1}^\\infty \\lambda^{n-1}\\left( \\sum_{k=1}^{n}\\gamma^{k-1}\\delta_{t+k} + O(\\alpha) \\right) \n",
        "\\\\ &=& \\underbrace{\\sum_{n=1}^\\infty \\sum_{k=1}^{n}(1-\\lambda)\\lambda^{n-1} \\gamma^{k-1}\\delta_{t+k}}_{(*)} + O(\\alpha)\n",
        "\\end{eqnarray}$$\n",
        "ここで、任意の2数列$\\{a_n\\}$、$\\{b_m\\}$間に成り立つ\n",
        "$$\n",
        "\\sum_{n=1}^N \\sum_{m=1}^n a_n b_m = \\sum_{m=1}^N\\sum_{n=m}^N a_n b_m\n",
        "$$\n",
        "の関係を利用して\n",
        "$$\\begin{eqnarray}\n",
        "(*) &=& \\sum_{k=1}^\\infty \\sum_{n=k}^\\infty(1-\\lambda)\\lambda^{n-1} \\gamma^{k-1}\\delta_{t+k}\n",
        "\\\\ &=& \\sum_{k=1}^{T-t} (1-\\lambda) \\lambda ^{k-1}\\frac{1}{1 -\\lambda} \\gamma^{k-1}\\delta_{t+k}\n",
        "\\\\ &=& \\sum_{k=1}^\\infty (\\lambda\\gamma)^{k-1}\\delta_{t+k} \n",
        "\\end{eqnarray}$$\n",
        "\n",
        "よって、TD($\\lambda$)誤差は\n",
        "$$\n",
        "G_t^{\\lambda} - Q_t(S_t,A_t) = \\sum_{k=1}^\\infty (\\lambda\\gamma)^{k-1}\\delta_{t+k} + O(\\alpha)\n",
        "$$\n",
        "のように、学習率$\\alpha$の一次の誤差を近似し$\\lambda\\gamma$を割引率とみなしたときの、$k=1,2,\\cdots$ステップ先での1stepTD誤差の割引級数和として展開できる.\n",
        "\n",
        "※ここで、エピソードの終端状態Tで有限にして議論しないと話が進まなくなったので、教科書の$\\lambda$-収益の定義から出発してもう一度同じような証明をやった上でこの続きに入っていきます."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmfPHstE59h2"
      },
      "source": [
        "**終端状態Tを考慮するとき:**\n",
        "\n",
        "- $\\lambda$-収益の定義式：\n",
        "$$\n",
        "G_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^{\\infty}\\lambda^{n-1}G_t^{(n)}\n",
        "$$\n",
        "について、終端状態Tを考慮して次のように展開できる：\n",
        "$$\\begin{eqnarray}\n",
        "(右辺)&=& (1 - \\lambda) \\sum_{n=1}^{T-t}\\lambda^{n-1}G_t^{(n)} + \\underbrace{(1 - \\lambda) \\sum_{n=T-t+1}^{\\infty}\\lambda^{n-1}G_t^{(n)}}_{(*)} \\\\\n",
        "(*) &=& (1 - \\lambda)\\sum_{n=T-t+1}^{\\infty}\\lambda^{n-1}G_t^{(n)}\n",
        "\\\\ &=& (1 - \\lambda) \\lambda^{T-t}\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_t^{(T-t+n)}\n",
        "\\\\ &=& (1 - \\lambda) \\lambda^{T-t}\\underbrace{\\sum_{n=1}^{\\infty}\\lambda^{n-1}}_{1 / (1 - \\lambda)}G_t^{(T-t)}\n",
        "\\\\ &=& \\lambda^{T-t} G_t^{(T-t)}\n",
        "\\end{eqnarray}$$\n",
        "ただし、終端状態T以降の任意の時点$T+k$における報酬$R_{T+k}$は0なので\n",
        "$$G_t^{(T-t+n)}=G_t^{(T-t)} \\quad(n=1,2,\\cdots)$$\n",
        "であることを用いた.\n",
        "- 以上より、\n",
        "$$\n",
        "G_t^{\\lambda} = (1 - \\lambda) \\sum_{n=1}^{T-t}\\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t} G_t^{(T-t)}\n",
        "$$\n",
        "\n",
        "- この両辺から\n",
        "$$\n",
        "Q_t(S_t,A_t) = \\left\\{ (1-\\lambda)\\sum_{n=1}^{T-t} \\lambda^{n-1} + \\lambda^{T-t} \\right\\} Q_t(S_t,A_t)\n",
        "$$\n",
        "を引いて\n",
        "\n",
        "$$\n",
        "G_t^{\\lambda} - Q_t(S_t,A_t) = (1-\\lambda)\\sum_{n=1}^{T-t} \\lambda^{n-1}\\left(G_t^{(n)} - Q_t(S_t,A_t)\\right) + \\lambda^{T-t}\\left(G_t^{(T-t)} - Q_t(S_t,A_t) \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl6N2HWRFv7V"
      },
      "source": [
        "$$\\begin{eqnarray}\n",
        "G_t^{\\lambda} - Q_t(S_t,A_t) \n",
        "&=& (1-\\lambda)\\sum_{n=1}^{T-t} \\lambda^{n-1}\\left(G_t^{(n)} - Q_t(S_t,A_t)\\right) + \\lambda^{T-t}\\left(G_t^{(T-t)} - Q_t(S_t,A_t) \\right)\n",
        "\\\\ &=& (1-\\lambda)\\sum_{n=1}^{T-t} \\lambda^{n-1}\\left( \\sum_{k=1}^{n}\\gamma^{k-1}\\delta_{t+k} + O(\\alpha) \\right) + \\lambda^{T-t}\\left( \\sum_{k=1}^{T-t}\\gamma^{k-1}\\delta_{t+k} + O(\\alpha) \\right)\n",
        "\\\\ &=& \\underbrace{\\sum_{n=1}^{T-t} \\sum_{k=1}^{n}(1-\\lambda)\\lambda^{n-1} \\gamma^{k-1}\\delta_{t+k}}_{(*)} + \\lambda^{T-t} \\sum_{k=1}^{T-t}\\gamma^{k-1}\\delta_{t+k} + O(\\alpha)\n",
        "\\end{eqnarray}$$\n",
        "- ここで、任意の2数列$\\{a_n\\}$、$\\{b_m\\}$間に成り立つ\n",
        "$$\n",
        "\\sum_{n=1}^N \\sum_{m=1}^n a_n b_m = \\sum_{m=1}^N\\sum_{n=m}^N a_n b_m\n",
        "$$\n",
        "の関係を利用して\n",
        "$$\\begin{eqnarray}\n",
        "(*) &=& \\sum_{k=1}^{T-t} \\sum_{n=k}^{T-t}(1-\\lambda)\\lambda^{n-1} \\gamma^{k-1}\\delta_{t+k}\n",
        "\\\\ &=& \\sum_{k=1}^{T-t} (1-\\lambda) \\frac{\\lambda^{k-1} - \\lambda^{T-t}}{1 -\\lambda} \\gamma^{k-1}\\delta_{t+k}\n",
        "\\\\ &=& \\sum_{k=1}^{T-t} (\\lambda^{k-1} - \\lambda^{T-t})\\gamma^{k-1}\\delta_{t+k}\n",
        "\\\\ &=& \\sum_{k=1}^{T-t} \\lambda^{k-1}\\gamma^{k-1}\\delta_{t+k} - \\lambda^{T-t}\\sum_{k=1}^{T-t}\\gamma^{k-1}\\delta_{t+k}\n",
        "\\end{eqnarray}$$\n",
        "と変形できる.　\n",
        "- よって、TD($\\lambda$)誤差は\n",
        "$$\n",
        "G_t^{\\lambda} - Q_t(S_t,A_t) = \\sum_{k=1}^{T-t} (\\lambda\\gamma)^{k-1}\\delta_{t+k} + O(\\alpha)\n",
        "$$\n",
        "のように、学習率$\\alpha$の一次の誤差を近似し$\\lambda\\gamma$を割引率とみなしたときの、$k=1,2,\\cdots,T-t$ステップ先での1stepTD誤差の割引級数和として展開できる."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwVDJjraPBde"
      },
      "source": [
        "以上より、1エピソードにおける各状態行動対$(s,a)$についてのTD($\\lambda$)誤差の総和は\n",
        "$$\\begin{eqnarray}\n",
        "\\sum_{t=0}^{T-1}\\left(G_t^{\\lambda} - Q_t(S_t,A_t)\\right)1(S_t=s,A_t=a) \n",
        "&=& \\sum_{t=0}^{T-1}\\left(\\sum_{k=1}^{T-t} (\\lambda\\gamma)^{k-1}\\delta_{t+k}+ O(\\alpha) \\right)1(S_t=s,A_t=a) \\\\\n",
        "(右辺) &=& \\left( \\sum_{t=0}^{T-1}\\sum_{k=1}^{T-t} (\\lambda\\gamma)^{k-1}\\delta_{t+k}+ O(\\alpha) \\right)1(S_t=s,A_t=a)\n",
        "\\\\ &=& \\sum_{t=0}^{T-1}\\sum_{k=0}^{T-1-t} (\\lambda\\gamma)^k\\delta_{t+k+1} 1(S_t=s,A_t=a) + O(\\alpha)\n",
        "\\\\ &=& \\sum_{t=0}^{T-1}\\sum_{k=t}^{T-1} (\\lambda\\gamma)^{k-t}\\delta_{k+1} 1(S_t=s,A_t=a) + O(\\alpha)\n",
        "\\\\ &=& \\sum_{k=0}^{T-1} \\delta_{k+1} \\sum_{t=0}^{k} (\\lambda\\gamma)^{k-t} 1(S_t=s,A_t=a) + O(\\alpha)\n",
        "\\\\ &=& \\sum_{k=0}^{T-1} \\delta_{k+1} E_{k+1}(s,a) + O(\\alpha) \\\\\n",
        "\\therefore \\sum_{t=0}^{T-1}\\left(G_t^{\\lambda} - Q_t(S_t,A_t)\\right)1(S_t=s,A_t=a) \n",
        "&=& \\sum_{k=0}^{T-1} \\delta_{k+1} E_{k+1}(s,a) + O(\\alpha)\n",
        "\\end{eqnarray}$$\n",
        "- ただし\n",
        "$$\\begin{eqnarray}\n",
        "E_{k+1}(s,a) &=& \\sum_{t=0}^{k} (\\lambda \\gamma)^{k-t} 1(S_t=s,A_t=a)\n",
        "\\\\ &=& 1(S_k=s,A_k=a) + (\\lambda \\gamma)^{1} 1(S_{k-1}=s,A_{k-1}=a) + \\cdots + (\\lambda \\gamma)^{k} 1(S_0=s,A_0=a)\n",
        "\\end{eqnarray}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIni9DgmVye9"
      },
      "source": [
        "- 以上より、TD($\\lambda$)法において、1つのエピソード内で、各状態行動対$(s,a)$について、時点$t$から$t+1$へのステップにおける$Q$関数の変化量$\\Delta Q_{t+1}(s,a)$について\n",
        "$$\n",
        "\\frac{1}{\\alpha}\\Delta Q_{t+1}(s,a) = \\left(G_t^{\\lambda} - Q_t(S_t,A_t)\\right)1(S_t=s,A_t=a) = \\delta_{k+1} E_{k+1}(s,a) + O(\\alpha)\n",
        "$$\n",
        "が成り立つ.\n",
        "- このことより、各時点$t$において全ての状態行動対$(s,a)$について\n",
        "$$\\begin{eqnarray}\n",
        "Q_{t+1}(s,a) &=& Q_t(s,a) + \\alpha \\left(G_t^{\\lambda} - Q_t(S_t,A_t)\\right)1(S_t=s,A_t=a)\n",
        "\\\\ &=&  Q_t(s,a) + \\alpha \\left( \\sum_{k=1}^{T-t} (\\lambda\\gamma)^{k-1}\\delta_{t+k} \\right) 1(S_t=s,A_t=a)\n",
        "\\\\ &=& Q_t(s,a) + \\alpha \\delta_{t+1} E_{t+1}(s,a) \n",
        "\\\\ E_{t+1}(s,a) &=& \\lambda \\gamma E_t(s,a) + 1(S_t=s,A_t=a)\n",
        "\\\\ \\delta_{t+1} &=& R_{t+1} + \\gamma Q_t(S_{t+1},A_{t+1}) - Q_t(S_t,A_t)\n",
        "\\end{eqnarray}$$\n",
        "のように更新することで、近似的にオンライン学習が可能になる.\n",
        "\n",
        "- TD($\\lambda$)についての意味の解釈は、個人的に、https://yamaimo.hatenablog.jp/entry/2015/12/12/200000 というブログが参考になりました."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5mu6PlFOrUr"
      },
      "source": [
        "$\\epsilon$-greedy法:\n",
        "- 環境モデルが未知で、$Q$関数が推定値にすいぎないとき、greedyな方策は必ずしも最適にはならない：\n",
        "$$\n",
        "a_{\\pi,s}^* \\neq \\mbox{argmax}_a Q(s,a)\n",
        "$$\n",
        "- 確率的なばらつきによって、最適行動の価値関数が過小評価されている可能性がある．\n",
        "- 確率$\\epsilon$で無作為に行動を選択し、確率$1-\\epsilon$でgreedy法により行動を選択する.\n",
        "$$\n",
        "\\pi(a|s) = \n",
        "\\left\\{\\begin{array}{ll}\n",
        "(1 - \\epsilon)/|A^*(s)| + \\epsilon/|A(s)|, & a \\in A^*(s) \\\\\n",
        "\\epsilon/|A(s)|, & \\mbox{otherwise}\n",
        "\\end{array}\\right.\\\\\n",
        " A^*(s) = \\left\\{ a^* \\in A(s)| a^* = \\mbox{argmax}_a Q(s,a)\\right\\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LjPeqPmWz6A"
      },
      "source": [
        "SARSA:\n",
        "- $\\epsilon$-greedy法により方策改善し、その方策に基いて時点$t+1$の行動$A_{t+1}$を生成しながら(0)TDにより$Q$関数を学習していく方法.\n",
        "- サンプリングが$(S_t, A_t, R_{t+1},S_{t+1},A_{t+1})$であることからSARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGBS5bVs8Yx3"
      },
      "source": [
        "Q学習:\n",
        "- TD(0)法のTD誤差の次時点の$Q$関数の最大値を選ぶようにする.\n",
        "$$\\begin{eqnarray}\n",
        "\\delta_{t+1} &=& R_{t+1} + \\gamma \\mbox{max}_{a'}Q_t(S_{t+1},a') - Q_t(S_{t},A_{t}) \\\\ Q_{t+1}(s,a) &=& Q_t(s,a) + \\alpha \\delta_{t+1}1(S_t=s,A_t=a)\n",
        "\\end{eqnarray}$$\n",
        "- TD誤差の推定における次行動の選択の仕方（推定方策）がgreedy\n",
        "- 方策の改善の仕方は$\\epsilon$-greedy\n",
        "- 方策オフ型：推定方策として挙動方策とは異なる方策を採用する学習方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blZ2VceF__vW"
      },
      "source": [
        "DQN (Double Q Network):\n",
        "- 各ステップで更新していく$Q$関数($Q_{main}$)とtarget値の推定に使う$Q$関数($Q_{target}$)を別々に用意して行う$Q$学習.\n",
        "- 一定ステップ毎に$Q_{target}$を$Q_{main}$と同期させて、目標値の推定値を更新.\n",
        "- $Q$関数はニューラルネットワークでモデル化：\n",
        "$$ Q: s \\in S \\rightarrow  x(s) \\in R^m \\rightarrow Q(s) = (Q(s,a_1), Q(s,a_2), \\cdots, Q(s,a_n))^T \\in R^n$$\n",
        " - ひとつの状態$s$に対して取り得る全ての行動$a \\in A$の行動価値関数$Q(s,a)$を全て一度に返すモデルにすることで計算コスト減.\n",
        " - target値内の行動価値関数の推定値は$\\mbox{max}_{a'}Q_{target}(s,a') = \\mbox{max}(Q_{target}(s))$ですぐに求められる.\n",
        " - 各ステップの$Q_{main}$の更新式は\n",
        " $$\n",
        " \\delta_{t+1} = R_{t+1} + \\mbox{max}_{A_{t+1}}Q_{target}(S_{t+1},A_{t+1}) - Q_{main}(S_t,A_t) \\\\\n",
        " \\left( \n",
        "   \\begin{array}{c} Q_{main}(s,a_1) \\\\ Q_{main}(s,a_2) \\\\ \\cdots \\\\ Q_{main}(s,a_n)\n",
        "   \\end{array}\\right) \\leftarrow \\left(\\begin{array}{c} Q_{main}(s,a_1) \\\\ Q_{main}(s,a_2) \\\\ \\cdots \\\\ Q_{main}(s,a_n) \\end{array}\\right) + \\alpha \\delta_{t+1}\\left( \n",
        "   \\begin{array}{c} 1(S_t=s,A_t=a_1) \\\\ 1(S_t=s,A_t=a_2) \\\\ \\cdots \\\\ 1(S_t=s,A_t=a_n)\n",
        "   \\end{array}\\right)\n",
        "$$\n",
        "と書ける.\n",
        "\n",
        " - 一定ステップごとに$Q_{target} \\leftarrow Q_{main}$により$Q_{target}$を更新.\n",
        "\n",
        "- $Q_{target}$を使うメリット：\n",
        "- reward clipping\n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6PbHGwzAnJ3"
      },
      "source": [
        "以上にみてきたように、行動価値関数の推定値$Q(s,a)$のテーブルを用意して推定していく方法は、状態空間や行動空間が高次元または連続になると破綻する.　そこで、次脳ような方策ベース手法を導入する："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hva4IfzsBEjo"
      },
      "source": [
        "方策ベース手法：\n",
        "- 方策をパラメータ$\\theta$を用いて推定（モデル化）し、目的関数$J(\\theta)$に基づき$\\theta$を改善していくことで方策を最適化していく:\n",
        "$$\\begin{align}\n",
        "\\pi(a|s,\\theta) \\\\\n",
        "\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
        "\\end{align}$$　\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yglo1iw0FMLO"
      },
      "source": [
        "ベルマン方程式\n",
        "$$\n",
        "\\begin{align}\n",
        "v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) q_\\pi(s,a)\n",
        "\\end{align}\n",
        "$$\n",
        "は方策$\\pi$を介して$\\theta$にのみ依存するとするとして両辺を$\\theta$で微分して整理すると\n",
        "$$\\begin{eqnarray}\n",
        "\\nabla_\\theta v_\\pi(s) &=& \\sum_{s' \\in S} d^\\pi(s,s') \\sum_{a \\in A} (\\nabla_\\theta \\pi(a|s',\\theta)) q_\\pi(s',a)\n",
        "\\end{eqnarray}$$\n",
        "を得る（詳細は教科書参照）.　ただし、\n",
        "$$\\begin{eqnarray}\n",
        "\\\\ d^\\pi(s,s') &=& \\sum_{k=0}^\\infty \\gamma^k [(P^\\pi )^k]_{ss'},\n",
        "\\\\ [P^\\pi]_{ss'} &=& P_{ss'}^\\pi = \\sum_{a \\in A} \\pi(a|s,\\theta) p(s'|s,a).\n",
        "\\end{eqnarray}$$\n",
        "ここで\n",
        "$$\n",
        "\\nabla_x \\log(f(x)) = \\sum_{i} \\frac{\\partial \\log(f(x))}{\\partial x_i}\n",
        "= \\sum_{i} \\frac{\\partial \\log(f(x))}{\\partial f(x)}\\frac{\\partial f(x)}{\\partial x_i} = \\frac{1}{f(x)} \\nabla_x f(x), \\\\\n",
        "\\nabla_x f(x) = f(x) \\nabla_x \\log (f(x))\n",
        "$$\n",
        "を利用して\n",
        "$$\n",
        "\\nabla_\\theta v_\\pi(s_0) = \\sum_{s \\in S} d^\\pi(s_0,s) \\sum_{a \\in A} \\pi(a|s,\\theta)(\\nabla_\\theta \\log \\left(\\pi(a|s,\\theta))\\right) q_\\pi(s,a)\n",
        "$$\n",
        "とできる.　したがって、目的関数を次のように設定して方策勾配を得られる.　\n",
        "$$\\begin{eqnarray}\n",
        "J(\\theta) &=& v_\\pi(s_0),\n",
        "\\\\ \\nabla_\\theta J(\\theta) &=& E_\\pi[\\left( \\nabla_\\theta \\log(\\pi(a|s,\\theta))\\right) q_\\pi(s,a)]\n",
        "\\end{eqnarray}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgC2yyDJRIoV"
      },
      "source": [
        "ベースライン関数の導入：\n",
        "- 状態$s$に依存し、行動$a$に依存しない任意の関数$b(s)$について次が成り立つ:\n",
        "$$\\begin{eqnarray}\n",
        "E_\\pi[\\left( \\nabla_\\theta \\log(\\pi(a|s,\\theta))\\right) b(s)]\n",
        "&=& \\sum_{s \\in S} d^\\pi(s_0,s) \\sum_{a \\in A} \\pi(a|s,\\theta)(\\nabla_\\theta \\log \\left(\\pi(a|s,\\theta))\\right) b(s)\n",
        "\\\\ &=& \\sum_{s \\in S} d^\\pi(s_0,s) b(s) \\sum_{a \\in A} \\nabla_\\theta \\pi(a|s,\\theta )\n",
        "\\\\ &=& \\sum_{s \\in S} d^\\pi(s_0,s) b(s) \\nabla_\\theta \\underbrace{\\sum_{a \\in A} \\pi(a|s,\\theta )}_{1:定数}\n",
        "\\\\ &=& 0\n",
        "\\end{eqnarray}$$\n",
        "よって\n",
        "$$\n",
        "E_\\pi[\\left( \\nabla_\\theta \\log(\\pi(a|s,\\theta))\\right) q_\\pi(s,a)] = E_\\pi[\\left( \\nabla_\\theta \\log(\\pi(a|s,\\theta))\\right) (q_\\pi(s,a) - b(s))]\n",
        "$$\n",
        "\n",
        "- ベースライン関数として価値関数$v_\\pi(s)$を選び、行動価値関数を価値関数でシフトした関数:\n",
        "$$\n",
        "a_\\pi(s,a) = q_\\pi(s,a) - v_\\pi(s)\n",
        "$$\n",
        "をアドバンテージ関数と呼ぶ. これは、方策$\\pi$、状態$s$のもとで行動$a$が平均と比べてどのくらい価値が高いかを表している.\n",
        "- 方策勾配定理における行動価値関数をアドバンテージ関数で置き換えるとバリアンスが低く抑えられた方策勾配が得られる:(?)\n",
        "$$\\begin{eqnarray}\n",
        "\\nabla_\\theta J(\\theta) &=& E_\\pi[\\left( \\nabla_\\theta \\log(\\pi(a|s,\\theta))\\right) (q_\\pi(s,a) - v_\\pi(s))]\n",
        "\\\\ & \\approx & \\frac{1}{T} \\sum_{t=0}^{T-1} \\nabla_\\theta (\\log (\\pi(A_t|S_t,\\theta)))(Q(S_t,A_t) - V(S_t))\n",
        "\\end{eqnarray}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l2l4jLNYRc7"
      },
      "source": [
        "- 以上の計算過程をまとめると、\n",
        "$$\\begin{eqnarray}\n",
        "\\nabla_\\theta J(\\theta) \n",
        "&=& \\nabla_\\theta v_\\pi(s)\n",
        "\\\\ &=& \\sum_{s \\in S} d^\\pi(s_0,s) \\sum_{a \\in A} \\pi(a|s,\\theta)(\\nabla_\\theta \\log \\left(\\pi(a|s,\\theta))\\right) q_\\pi(s,a)\n",
        "\\\\ &=& \\sum_{s \\in S} d^\\pi(s_0,s) \\sum_{a \\in A} \\pi(a|s,\\theta)(\\nabla_\\theta \\log \\left(\\pi(a|s,\\theta))\\right) (q_\\pi(s,a) - v_\\pi(s))\n",
        "\\\\ &=& E_\\pi[\\left( \\nabla_\\theta \\log(\\pi(a|s,\\theta))\\right) (q_\\pi(s,a) - v_\\pi(s))]\n",
        "\\\\ & \\approx & \\frac{1}{T} \\sum_{t=0}^{T-1} \\nabla_\\theta (\\log (\\pi(A_t|S_t,\\theta)))(Q(S_t,A_t) - V(S_t))\n",
        "\\end{eqnarray}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uems0TIPYRX-"
      },
      "source": [
        "- 問題となるのは、方策ベースを使用するような高次元または連続な行動空間において、$Q$テーブルの使用は非現実的.\n",
        "- 方策改善のための方策のモデル化に加えて、方策評価のための行動価値関数$Q$のモデル化も必要になってくる.\n",
        "- 行動価値関数の代わりに価値関数をモデル化:$V_\\omega(s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn6FnJxEdJfN"
      },
      "source": [
        "Actor-Critic法:\n",
        "- agentが担う方策改善と方策評価の機能を分離し、個々をモデル化する方法\n",
        " - Actor: 方策$\\pi(a|s,\\theta)$をモデル化し、方策勾配に基づきパラメータ$\\theta$を更新(=方策の改善)\n",
        " - Critic: 価値関数$V_\\omega(s)$をモデル化し、TD誤差に基づきパラメータ$\\omega$を更新(=価値関数の推定精度の向上)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-9zFugbgcMy"
      },
      "source": [
        "Actor:\n",
        "- 目的関数の設定:\n",
        " - 方策を最適化したい\n",
        " - 方策勾配を上っていった頂上に着きたい\n",
        " - 最小化すべき目的関数は、$\\theta$で微分したら方策勾配の$-1$倍になるもの:\n",
        " $$\n",
        " L_{actic}(\\theta) = - J(\\theta) \\approx - \\frac{1}{T}\\sum_{t=0}^{T-1}(\\log\\pi(A_t|S_t,\\theta)) (Q(S_t,A_t) - V_\\omega(S_t)) $$\n",
        " - このままでは、高次元または連続な行動空間においてやっかいな$Q$を含んでいる.\n",
        " そこで、\n",
        " $$\n",
        " \\delta(s,r,s') = r + \\gamma v_\\pi(s') - v_\\pi(s)\n",
        " $$\n",
        " の期待値は\n",
        " $$\\begin{eqnarray}\n",
        "E_\\pi[\\delta(s,r,s')] &=& E_\\pi[r + \\gamma v_\\pi(s') - v_\\pi(s)]\n",
        "\\\\ &=& E_\\pi[q_\\pi(s,a) - v_\\pi(s)]\n",
        " \\end{eqnarray}$$\n",
        " となるという事実を使って$Q$を消去する.　\n",
        " \n",
        " - すなわち、\n",
        " $$ Q(S_t,A_t) - V_\\omega(S_t) $$\n",
        " の期待値は\n",
        " $$ \\delta_{t+1}(\\omega) = R_{t+1} + \\gamma V_\\omega(S_{t+1}) - V_\\omega(S_{t}) $$\n",
        " の期待値と等しくなることを利用して\n",
        " $$ Q(S_t,A_t) - V_\\omega(S_t) \\approx \\delta_{t+1}(\\omega)$$\n",
        " でとして\n",
        " $$\n",
        " L_{actic}(\\theta) = - J(\\theta) \\approx - \\frac{1}{T}\\sum_{t=0}^{T-1}(\\log\\pi(A_t|S_t,\\theta)) \\delta_{t+1}(\\omega) $$\n",
        " と近似的にかける.　\n",
        "\n",
        " これにより、高次元または連続な行動空間では発散してしまう$Q(S_t,A_t)$を使わずに方策勾配を求められるようになった."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mce_UvZ7fN5v"
      },
      "source": [
        "Critic:\n",
        "- 目的関数の設定　\n",
        " - 最小化すべき目的関数はTD誤差の二乗和:\n",
        " $$\n",
        " L_{critic} = \\sum_{t=0}^{T-1}|\\delta_{t+1}(w)|^2,\\\\\n",
        " \\delta_{t+1} = R_{t+1} + \\gamma V_\\omega (S_{t+1}) - V_\\omega(S_t)\n",
        " $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6heA79TYRtad"
      },
      "source": [
        "教科書にあったこのような記述だけではCriticの目的関数の背景が良く分からなかったため、以下にDavid Silver氏によるRL Course - Lecture 6のValue Function Approximationの説明を参考にまとめました：\n",
        "\n",
        "価値関数モデルの更新、すなわち、パラメータ$w$の更新：\n",
        "- 目的関数：平均二乗誤差（教科書とは正負が逆になっていることに注意.　こちらでは目的関数を最大化するように書くことで、結果の式が上の方で述べた価値関数推定値の更新式を一般化できるようにまとめてある.）\n",
        "$$\n",
        "L(w) = - \\frac{1}{2} E_\\pi[(v_\\pi(s) - \\hat v (s,w))^2]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\nabla_w L(w)  = (v_\\pi(s) - \\hat v (s,w)) \\nabla_w \\hat v(s,w)\n",
        "$$\n",
        "- よって、パラメータ$w$の更新式は:\n",
        "$$\\begin{eqnarray}\n",
        "w &\\leftarrow& w + \\alpha \\nabla_w L(w) \\\\\n",
        "  &=& w + \\alpha (v_\\pi(s) - \\hat v (s,w)) \\nabla_w \\hat v(s,w) \\\\\n",
        "  &=& w + \\alpha \\delta (w) \\nabla_w \\hat v(s,w) \\\\\n",
        "\\delta (w) &=& v_\\pi(s) - \\hat v (s,w)\n",
        "\\end{eqnarray}$$\n",
        "\n",
        "- ただし、予測を行う際には当然、価値関数の真の値$v_\\pi(s)$は分からないので、他の目標値で代替：\n",
        " - $G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t-1} R_{T}$\n",
        " - $G_t^{(1)} = R_{t+1} + \\hat v (s,w)$\n",
        "   - $\\delta (w) = \\delta_{t+1}(w) = R_{t+1} + \\gamma \\hat v(S_{t+1},w) - v(S_t,w)$ ：1stepTD誤差\n",
        " - $G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\hat v(s,w)$\n",
        "   - $\\delta(w) = \\delta_{t+1}^{(n)}(w) = \\delta_{t+1}(w) + \\gamma \\delta_{t+2}(w) + \\cdots + \\gamma^{n-1} \\delta_{t+n}(w)$：n-stepTD誤差\n",
        "   - $\\delta_{t+k}(w) = R_{t+k} + \\gamma \\hat v (S_{t+k},w) - \\hat v (S_{t+k-1},w)$\n",
        " - $G_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)}$\n",
        "   - $\\delta(w) = \\delta_{t+1}^\\lambda (w) = \\sum_{k=1}^{T-t} (\\lambda \\gamma)^{k-1}\\delta_{t+k}(w)$：TD($\\lambda$)誤差(forward)\n",
        "\n",
        "- このうち、目標値に$G_t^{(1)}$を用いているのが教科書のCritic.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5NSS-MPmJjn"
      },
      "source": [
        "以下では、価値関数の近似方法の主要な手段（線形近似、ニューラルネットワークモデル）について簡単にまとめました."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6_4_UrfKK9R"
      },
      "source": [
        "価値関数の線形モデル：\n",
        "- 価値関数を線形モデルで近似すると、次のように表せる：\n",
        "$$\\begin{eqnarray}\n",
        "x(s) &=& (x_1(s),x_2(s), \\cdots, x_n(s))^T \\\\\n",
        "w &=& (w_1,w_2, \\cdots, w_n)^T \\\\\n",
        "\\hat v(s,w) &=& w^Tx(s) = \\sum_{i=1}^n w_i x_i(s)\n",
        "\\end{eqnarray}$$\n",
        "- よって、線形モデルにおいては\n",
        "$$\\nabla_w \\hat v(s,w) = x(s)$$\n",
        "よって更新式は $$w \\leftarrow w + \\alpha (v_\\pi(s) - \\hat v (s,w))x(s)$$となる.\n",
        "- table  lookupは価値関数線形近似の特別な場合である:\n",
        "$$\n",
        " x(s) = \\left( \n",
        "   \\begin{array}{c} 1(s=s_1) \\\\ 1(s=s_2) \\\\ \\cdots \\\\ 1(s=s_n)\n",
        "   \\end{array}\\right) \\\\\n",
        "   \\hat v (s,w) = \\left( \n",
        "   \\begin{array}{c} 1(s=s_1) \\\\ 1(s=s_2) \\\\ \\cdots \\\\ 1(s=s_n)\n",
        "   \\end{array}\\right) \\cdot \\left( \n",
        "   \\begin{array}{c} w_1 \\\\ w_2 \\\\ \\cdots \\\\ w_n\n",
        "   \\end{array}\\right)\n",
        "$$\n",
        "$w_1, w_2, \\cdots,w_n$はそれぞれ$V(s_1),V(s_2),\\cdots,V(s_n)$を表す.\n",
        "- よって更新式は：\n",
        "$$\n",
        "\\left( \n",
        "   \\begin{array}{c} V(s_1) \\\\ V(s_2) \\\\ \\cdots \\\\ V(s_n)\n",
        "   \\end{array}\\right) \\leftarrow \\left(\\begin{array}{c} V(s_1) \\\\ V(s_2) \\\\ \\cdots \\\\ V(s_n) \\end{array}\\right) + \\alpha (v_\\pi(s) - \\hat v (s,w))\\left( \n",
        "   \\begin{array}{c} 1(s=s_1) \\\\ 1(s=s_2) \\\\ \\cdots \\\\ 1(s=s_n)\n",
        "   \\end{array}\\right)\n",
        "$$\n",
        "とまとめられる.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMa_hDhBO1ws"
      },
      "source": [
        "価値観数のニューラルネットワークモデル：\n",
        "- ニューラルネットワークのパラメータ全体をまとめて$w$とすると:\n",
        "$$\n",
        "\\hat v(s,w) = \\mbox{Model}_w^{NN}(x(s))\n",
        "$$\n",
        "状態$s$の特徴量は$x(s) \\in R^n$で表され、それがニューラルネットワークモデル$y= \\mbox{Model}_w^{NN}(x)$に放り込まれて、状態$s$の価値関数の推定値$\\hat v (s,w) = y$が返される.\n",
        "\n",
        "- ニューラルネットワークモデルのパラメータの更新は、上で述べたような更新式で行う.　すなわち、目的関数$L$のパラメータ$w$による勾配$\\nabla_w L$に応じ、目的関数を最大化（誤差を小さく）するように、ステップ幅$\\alpha$で$w$を更新.\n",
        "\n",
        "- 例えば、DeepMind社によるインベーダーゲームなどAtariゲームの強化学習では、関数近似に畳み込みニューラルネットワークCNNを用いることで、学習を重ねるうちにネットワークがゲーム画面（状態の生情報）から特徴を自動的にうまく抽出できるようになって、その特徴に基づいて価値関数を推定するようになっていくため上手く学習できるのだと考えられます."
      ]
    }
  ]
}
